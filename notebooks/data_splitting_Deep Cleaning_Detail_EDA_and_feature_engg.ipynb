{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e039833",
   "metadata": {},
   "source": [
    "### Step 2. The Split (The Firewall): Split the data \n",
    "\n",
    "First will split data and then We will follow below steps to handle 'unknown' values from \"job', 'education','contact' and 'poutcome' columns.\n",
    "First, calculate the percentage of \"unknown\" values in each column to understand the extent of the issue.\n",
    "\n",
    "\"Pro-Tip\" for Banking\n",
    "When perform Train-Test Split, always use Stratification.\n",
    "\n",
    "In our bank-full dataset, \"Success\" (y=1) is likely rare (around 11-12%). If a random split done without stratification, it might end up with 15% success in Train and only 5% in Test. This will make evaluation metrics (like Precision/Recall) completely unreliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26ccc78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the training data we processed earlier\n",
    "df = pd.read_csv('../data/processed/df_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22ebf16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e60a941d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (36168, 20)\n",
      "Test set size: (9043, 20)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Always stratify by the target variable 'y' in banking\n",
    "X = df.drop('y', axis=1)\n",
    "y = df['y']\n",
    "# 80/20 Split with Stratification (keeps failure ratio consistent)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474405ef",
   "metadata": {},
   "source": [
    "Now, we have\n",
    "\n",
    "Train data = X_train, y_train \n",
    "Test data = X_test, y_test\n",
    "\n",
    "Feature Engineering\n",
    "\n",
    "Now, will perform Data cleaning, feature engineering and EDA only on train data : X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d6f96cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.DataFrame'>\n",
      "Index: 36168 entries, 24001 to 44229\n",
      "Data columns (total 21 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   age              36168 non-null  int64  \n",
      " 1   job              36168 non-null  str    \n",
      " 2   marital          36168 non-null  str    \n",
      " 3   education        36168 non-null  str    \n",
      " 4   default          36168 non-null  str    \n",
      " 5   balance          36168 non-null  int64  \n",
      " 6   housing          36168 non-null  str    \n",
      " 7   loan             36168 non-null  str    \n",
      " 8   contact          36168 non-null  str    \n",
      " 9   day              36168 non-null  int64  \n",
      " 10  month            36168 non-null  str    \n",
      " 11  duration         36168 non-null  int64  \n",
      " 12  campaign         36168 non-null  int64  \n",
      " 13  pdays            36168 non-null  int64  \n",
      " 14  previous         36168 non-null  int64  \n",
      " 15  poutcome         36168 non-null  str    \n",
      " 16  total_contacts   36168 non-null  int64  \n",
      " 17  call_efficiency  36168 non-null  float64\n",
      " 18  multiple_loans   36168 non-null  int64  \n",
      " 19  is_new_customer  36168 non-null  int64  \n",
      " 20  y                36168 non-null  str    \n",
      "dtypes: float64(1), int64(10), str(10)\n",
      "memory usage: 6.1 MB\n"
     ]
    }
   ],
   "source": [
    "# Combine X and y back together for cleaning, featurization, EDA and Modeling storage\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Save to processed directory\n",
    "# train_full.to_csv('../data/processed/train_processed.csv', index=False)\n",
    "# test_full.to_csv('../data/processed/test_processed.csv', index=False)\n",
    "\n",
    "# print(\"Data cleaning and feature engineering complete. Files saved to data/processed/.\")\n",
    "\n",
    "df_train.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd1a291",
   "metadata": {},
   "source": [
    "### Part B Data Cleaning only on train data after spliting data in to train and test.\n",
    "\n",
    "1. Impute: Replace unknown in job or education with the mode.\n",
    "2. Handle Outliers: Cap the extreme balance values (Capping/Winsorization).\n",
    "\n",
    "**Step 3. Statistical Modeling Cleaning: Post-Split**\n",
    "\n",
    "Now in the Lab.\n",
    "\n",
    "- Imputation/Scaling: You calculate the \"Standard\" (Mean/Mode) from the Train set. If the Test set has a missing value, you fill           it with the Train mode. This mimics the real world, where you use past knowledge to handle new, incomplete information.\n",
    "\n",
    "- Handling Outliers\n",
    "\n",
    "- Bivariate EDA & Heatmaps: By doing this only on the training set, you ensure that your decision to keep or drop a feature is based only on the data the model is allowed to learn from.\n",
    "\n",
    "**Do these AFTER splitting:**\n",
    "\n",
    "1. Handling Outliers: Defining what an outlier is based on the training distribution.\n",
    "2. Imputation: Filling missing values using the mean, median, or mode of the training set only.\n",
    "3. Scaling/Normalization: If you normalize based on the global maximum, you've leaked the range of the test set into your training process.\n",
    "4. EDA: Visualizing correlations and distributions to decide which features to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dfb1a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job            234\n",
      "marital          0\n",
      "education     1482\n",
      "default          0\n",
      "housing          0\n",
      "loan             0\n",
      "contact      10386\n",
      "month            0\n",
      "poutcome     29589\n",
      "y                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 4. Impute: Replace unknown in job or education with the mode.\n",
    "# selected only categorical columns\n",
    "\n",
    "categorical_columns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome', 'y']\n",
    "\n",
    "# count unknown values in each column\n",
    "\n",
    "unknown_counts = df_train[categorical_columns].apply(lambda col: (col == \"unknown\").sum())\n",
    "\n",
    "print(unknown_counts) # the output will give unknown counts for each categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37f8ed64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: job, Unknown Values: 234, Percentage: 0.52%\n",
      "Column: education, Unknown Values: 1482, Percentage: 3.28%\n",
      "Column: poutcome, Unknown Values: 29589, Percentage: 65.45%\n",
      "Column: contact, Unknown Values: 10386, Percentage: 22.97%\n"
     ]
    }
   ],
   "source": [
    "# # Calculate the percentage of 'unknown' values in each relevant column\n",
    "columns_with_unknowns = ['job', 'education', 'poutcome', 'contact']\n",
    "for col in columns_with_unknowns:\n",
    "    unknown_count = df_train[df_train[col] == 'unknown'].shape[0]\n",
    "    total_count = df.shape[0]\n",
    "    percentage = (unknown_count / total_count) * 100\n",
    "    print(f\"Column: {col}, Unknown Values: {unknown_count}, Percentage: {percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7231be",
   "metadata": {},
   "source": [
    "**job and education:**  These are likely important features for predicting the target.\n",
    "the percentage of unknown values in 'job' and 'education' are 0.52% and 3.28%. which are very low.\n",
    "\n",
    "We will replace 'unknown' rows with mode value of that column.\n",
    "\n",
    "**contact and poutcome:**\n",
    "\n",
    "the percentage of unknown values in 'contact' and 'poutcome' are 0.22.97% and 65.45%. which are very low.\n",
    "\n",
    "'poutcome' columns have a high proportion (>30%) of \"unknown\" values and if their **impact** on the target variable (y) seems minimal, then we can remove them entirely.\n",
    "\n",
    "Let's check impact of 'contact' and 'potcome' on the target variable(y).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2601fb31",
   "metadata": {},
   "source": [
    "**Perform Chi-Square Test (Categorical Association)**\n",
    "\n",
    "A chi-square test can help determine whether there is a statistically significant association between the column (contact or poutcome) and the target variable (y).\n",
    "\n",
    "Interpretation:\n",
    "A p-value < 0.05 indicates a statistically significant relationship between the column and the target variable (y).\n",
    "A high p-value (>0.05) suggests the column has little impact on the target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65cd644b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contact - Chi-square p-value: 2.2216425534984923e-177\n",
      "Poutcome - Chi-square p-value: 0.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Create contingency tables for 'contact' and 'poutcome'\n",
    "contact_table = pd.crosstab(df_train['contact'], df_train['y'])\n",
    "poutcome_table = pd.crosstab(df_train['poutcome'], df_train['y'])\n",
    "\n",
    "# Perform chi-square test\n",
    "contact_chi2, contact_p, _, _ = chi2_contingency(contact_table)\n",
    "poutcome_chi2, poutcome_p, _, _ = chi2_contingency(poutcome_table)\n",
    "\n",
    "print(f\"Contact - Chi-square p-value: {contact_p}\")\n",
    "print(f\"Poutcome - Chi-square p-value: {poutcome_p}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92dd692",
   "metadata": {},
   "source": [
    "**Perform Chi-Square Test (Categorical Association)**\n",
    "\n",
    "A chi-square test can help determine whether there is a statistically significant association between the column (contact or poutcome) and the target variable (y).\n",
    "\n",
    "Interpretation:\n",
    "A p-value < 0.05 indicates a statistically significant relationship between the column and the target variable (y).\n",
    "A high p-value (>0.05) suggests the column has little impact on the target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "110e4b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is significant association with  2.2216425534984923e-177\n",
      "there is significant association with  0.0\n"
     ]
    }
   ],
   "source": [
    "check= [contact_p, poutcome_p] \n",
    "\n",
    "for col in check:\n",
    "    if col >= 0.05:\n",
    "        print(\"there is no association with\", col)\n",
    "    else:\n",
    "        print(\"there is significant association with \", col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52844178",
   "metadata": {},
   "source": [
    "Conclusion: we will retain the 'contact' and 'poutcome' columns from the data df_train. \n",
    "We have two options: \n",
    "1. continue as is with 'unkown' values\n",
    "2. replace 'unknown' values by mode value of that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16ff7f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode value of contact :  cellular\n",
      "mode value of poutcome:  unknown\n",
      "mode value of job :  blue-collar\n",
      "mode value of education:  secondary\n"
     ]
    }
   ],
   "source": [
    "# finding mode for 'contact' and 'potcome'\n",
    "mode_contact = df_train['contact'].mode()[0]\n",
    "mode_poutcome = df_train['poutcome'].mode()[0]\n",
    "\n",
    "print(\"mode value of contact : \", mode_contact)\n",
    "print(\"mode value of poutcome: \", mode_poutcome)\n",
    "\n",
    "# finding mode for 'job' and 'education'\n",
    "mode_job = df_train['job'].mode()[0]\n",
    "mode_education = df_train['education'].mode()[0]\n",
    "print(\"mode value of job : \", mode_job)\n",
    "print(\"mode value of education: \", mode_education)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b02d7896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning 'unknown' values imputed with relevant mode value.\n"
     ]
    }
   ],
   "source": [
    "# Replacing 'unknow' values by relevant column's mode value\n",
    "# We will keep poutcome \"unknowm\" values as is because the model will learn that \"unknown\" actually means \"new customer.\"\n",
    "df_cleaned=df_train\n",
    "for col in ['job', 'education','contact']:\n",
    "    mode_value = df_train[col].mode()[0]\n",
    "    df_cleaned[col] = df_cleaned[col].replace('unknown',mode_value)\n",
    "    #print(df_cleaned[col])\n",
    "\n",
    "\n",
    "print(\"Cleaning 'unknown' values imputed with relevant mode value.\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83f78ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation complete using Training Set statistics.\n"
     ]
    }
   ],
   "source": [
    "# # 1. Identify columns for Mode Imputation\n",
    "# impute_cols = ['job', 'education']\n",
    "\n",
    "# for col in impute_cols:\n",
    "#     # Calculate mode ONLY on Training set\n",
    "#     train_mode = X_train[col].mode()[0]\n",
    "    \n",
    "#     # Replace 'unknown' in both sets\n",
    "#     X_train[col] = X_train[col].replace('unknown', train_mode)\n",
    "#     X_test[col] = X_test[col].replace('unknown', train_mode)\n",
    "\n",
    "# 2. Rename 'unknown' in poutcome to keep the signal\n",
    "# This prevents 'unknown' from being treated as a missing value\n",
    "df_train['poutcome'] = df_train['poutcome'].replace('unknown', 'other_outcome')\n",
    "X_test['poutcome'] = X_test['poutcome'].replace('unknown', 'other_outcome')\n",
    "\n",
    "print(\"Imputation complete using Training Set statistics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3ca313",
   "metadata": {},
   "source": [
    "**Professional Outlier Handling: Capping**\n",
    "\n",
    "Handle Outliers: Cap the extreme balance values (Capping/Winsorization).\n",
    "\n",
    "Logic: don't just delete the outliers (that way we may lose too much data). Instead, cap them at the 99th percentile. This keeps the \"High Value\" signal without letting the €102,127 balance break your model.\n",
    "\n",
    "Capping vs. Deleting: In banking, we rarely delete outliers like a €102k balance. That person is a \"High Net Worth\" client! By Capping (using .clip()), we keep the customer in the dataset but prevent their high balance from \"pulling\" the average too far away from the typical customer.\n",
    "\n",
    "we want our \"Bivariate\" plots to be accurate. If we don't cap outliers first, our scatter plots will look like a single dot because of the scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3994897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers capped at 99th percentile for balance, duration, and campaign.\n",
      "Cleaning Complete: outliers are capped and unkown value handled. \n",
      "Data is ready for Featurization(Feature Engineering) and EDA\n"
     ]
    }
   ],
   "source": [
    "# 5. Handle Outliers (Capping / Winsorization)\n",
    "#  Handle Outliers (capping): Cap the extreme balance values (Capping/Winsorization).\n",
    "for col in ['balance', 'duration', 'campaign']:\n",
    "    upper_limit = df_train[col].quantile(0.99)\n",
    "    # We cap the values at the 99th percentile\n",
    "    df_train[col] = np.where(df_train[col] > upper_limit, upper_limit, df_train[col])\n",
    "    #or \n",
    "    # df_train[col] = df_train[col].clip(upper=upper_limit)\n",
    "\n",
    "\n",
    "print(\"Outliers capped at 99th percentile for balance, duration, and campaign.\")\n",
    "print(\"Cleaning Complete: outliers are capped and unkown value handled. \\nData is ready for Featurization(Feature Engineering) and EDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d54642a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of        age          job   marital  education default  balance housing loan  \\\n",
      "24001   36   technician  divorced  secondary      no    861.0      no   no   \n",
      "43409   24      student    single  secondary      no   4126.0      no   no   \n",
      "20669   44   technician    single  secondary      no    244.0     yes   no   \n",
      "18810   48   unemployed   married  secondary      no      0.0      no   no   \n",
      "23130   38   technician   married  secondary      no    257.0      no   no   \n",
      "...    ...          ...       ...        ...     ...      ...     ...  ...   \n",
      "17958   50  blue-collar   married  secondary      no    917.0     yes   no   \n",
      "15941   36       admin.    single  secondary      no     22.0     yes   no   \n",
      "16952   45  blue-collar   married  secondary      no     79.0     yes   no   \n",
      "34781   27   management    single   tertiary      no   2559.0     yes   no   \n",
      "44229   60       admin.   married  secondary      no    478.0      no   no   \n",
      "\n",
      "         contact  day  ... duration  campaign  pdays  previous       poutcome  \\\n",
      "24001  telephone   29  ...    140.0       2.0     -1         0  other_outcome   \n",
      "43409   cellular    5  ...    907.0       4.0    185         7        failure   \n",
      "20669   cellular   12  ...   1272.0       4.0     -1         0  other_outcome   \n",
      "18810  telephone   31  ...     35.0      11.0     -1         0  other_outcome   \n",
      "23130   cellular   26  ...     57.0      10.0     -1         0  other_outcome   \n",
      "...          ...  ...  ...      ...       ...    ...       ...            ...   \n",
      "17958   cellular   30  ...     58.0       2.0     -1         0  other_outcome   \n",
      "15941   cellular   22  ...     77.0       5.0     -1         0  other_outcome   \n",
      "16952   cellular   25  ...     98.0       1.0     -1         0  other_outcome   \n",
      "34781   cellular    6  ...    227.0       1.0     -1         0  other_outcome   \n",
      "44229   cellular   19  ...    173.0       2.0    311         8        failure   \n",
      "\n",
      "      total_contacts  call_efficiency  multiple_loans  is_new_customer    y  \n",
      "24001              2        46.666667               0                1   no  \n",
      "43409             11       181.400000               0                0  yes  \n",
      "20669              4       347.000000               0                1  yes  \n",
      "18810             11         2.916667               0                1   no  \n",
      "23130             10         5.181818               0                1   no  \n",
      "...              ...              ...             ...              ...  ...  \n",
      "17958              2        19.333333               0                1   no  \n",
      "15941              5        12.833333               0                1   no  \n",
      "16952              1        49.000000               0                1   no  \n",
      "34781              1       113.500000               0                1   no  \n",
      "44229             10        57.666667               0                0  yes  \n",
      "\n",
      "[36168 rows x 21 columns]>\n",
      "<class 'pandas.DataFrame'>\n",
      "Index: 36168 entries, 24001 to 44229\n",
      "Data columns (total 21 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   age              36168 non-null  int64  \n",
      " 1   job              36168 non-null  str    \n",
      " 2   marital          36168 non-null  str    \n",
      " 3   education        36168 non-null  str    \n",
      " 4   default          36168 non-null  str    \n",
      " 5   balance          36168 non-null  float64\n",
      " 6   housing          36168 non-null  str    \n",
      " 7   loan             36168 non-null  str    \n",
      " 8   contact          36168 non-null  str    \n",
      " 9   day              36168 non-null  int64  \n",
      " 10  month            36168 non-null  str    \n",
      " 11  duration         36168 non-null  float64\n",
      " 12  campaign         36168 non-null  float64\n",
      " 13  pdays            36168 non-null  int64  \n",
      " 14  previous         36168 non-null  int64  \n",
      " 15  poutcome         36168 non-null  str    \n",
      " 16  total_contacts   36168 non-null  int64  \n",
      " 17  call_efficiency  36168 non-null  float64\n",
      " 18  multiple_loans   36168 non-null  int64  \n",
      " 19  is_new_customer  36168 non-null  int64  \n",
      " 20  y                36168 non-null  str    \n",
      "dtypes: float64(4), int64(7), str(10)\n",
      "memory usage: 7.1 MB\n"
     ]
    }
   ],
   "source": [
    "print(df_train.head)\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba53cb6",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "In a professional banking workflow, the order of Feature Engineering versus Bivariate EDA is a bit of a \"chicken and egg\" situation, but there is a preferred path for efficiency.\n",
    "\n",
    "The industry-standard approach is to do Feature Engineering before Bivariate EDA.\n",
    "\n",
    "Why this order?\n",
    "\n",
    "If you create a feature like balance_per_campaign or age_groups first, you can include them in your Bivariate EDA and Heatmaps. This allows you to see if your newly created features actually have a stronger relationship with the target (y) than the raw data did.\n",
    "\n",
    "Standard: Create new columns that are derived from row-level logic (e.g., balance_per_campaign or age_groups).\n",
    "\n",
    "Why: These are based on business rules, not statistical distributions.\n",
    "\n",
    "Status: Correct. You did this before the split.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9397055f",
   "metadata": {},
   "source": [
    "**Summary Checklist for our Notebook:**\n",
    "\n",
    "done: Cleaning: Fix the data (Mode imputation, Outlier capping).\n",
    "\n",
    "current: Feature Engineering: Add new columns (derived from your SQL logic).\n",
    "\n",
    "Next: EDA (Part B): Compare variables to y (Bivariate) and check correlations.\n",
    "\n",
    "Split: Separate your data.\n",
    "\n",
    "Scale: Normalize the numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d5b2b2",
   "metadata": {},
   "source": [
    "# Part B Exploratory Data Analysis \n",
    "\n",
    "Goal: Gain insights into the dataset and understand relationships between variables.\n",
    "\n",
    "we are now in the \"Discovery Phase.\" In a banking context, Bivariate EDA isn't just about pretty pictures; it’s about finding Profitability Signals—identifying which types of customers are most likely to say \"Yes.\"\n",
    "\n",
    "Here is the industrial suite of Bivariate EDA, focusing on your engineered features and core banking variables.Actions:\n",
    "- Summarize numerical and categorical variables.\n",
    "- Visualize distributions, correlations, and trends.\n",
    "- Identify outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077538f4",
   "metadata": {},
   "source": [
    "1. Numerical Correlation (The \"Opportunity Map\")\n",
    "First, we look at the Heatmap. This tells us which variables move in sync with the target y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14891003",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['y'], dtype='str')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Only correlate numerical columns with the target\u001b[39;00m\n\u001b[32m      6\u001b[39m correlation_matrix = df_train.select_dtypes(include=[\u001b[33m'\u001b[39m\u001b[33mnumber\u001b[39m\u001b[33m'\u001b[39m]).corr()\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m sns.heatmap(\u001b[43mcorrelation_matrix\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43my\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m.sort_values(by=\u001b[33m'\u001b[39m\u001b[33my\u001b[39m\u001b[33m'\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m), \n\u001b[32m      8\u001b[39m             annot=\u001b[38;5;28;01mTrue\u001b[39;00m, cmap=\u001b[33m'\u001b[39m\u001b[33mcoolwarm\u001b[39m\u001b[33m'\u001b[39m, vmin=-\u001b[32m1\u001b[39m, vmax=\u001b[32m1\u001b[39m)\n\u001b[32m      9\u001b[39m plt.title(\u001b[33m\"\u001b[39m\u001b[33mCorrelation of Features with Subscription (y)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m plt.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:4384\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4382\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4383\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4384\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4386\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4387\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:6302\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6299\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6300\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6302\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6304\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6305\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6306\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:6352\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6350\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[32m   6351\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6352\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6354\u001b[39m     not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m   6355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"None of [Index(['y'], dtype='str')] are in the [columns]\""
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Only correlate numerical columns with the target\n",
    "correlation_matrix = df_train.select_dtypes(include=['number']).corr()\n",
    "sns.heatmap(correlation_matrix[['y']].sort_values(by='y', ascending=False), \n",
    "            annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title(\"Correlation of Features with Subscription (y)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85eb96f",
   "metadata": {},
   "source": [
    "2. Categorical Analysis (The \"Customer Profile\")\n",
    "We want to see the Subscription Rate across different categories. This is where your structural features (like job_type or quarter) will shine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b47682cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not interpret value `job_type` for `x`. An entry with this name does not appear in `data`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Test it on your job_type and your new financial flags\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mjob_type\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mquarter\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmultiple_loans\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mis_new_customer\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[43mplot_sub_rate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mplot_sub_rate\u001b[39m\u001b[34m(column, dataset)\u001b[39m\n\u001b[32m      3\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Calculate percentage of 'yes' per category\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43msns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbarplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43my\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpalette\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mviridis\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrorbar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#ci=None)\u001b[39;00m\n\u001b[32m      6\u001b[39m plt.axhline(dataset[\u001b[33m'\u001b[39m\u001b[33my\u001b[39m\u001b[33m'\u001b[39m].mean(), color=\u001b[33m'\u001b[39m\u001b[33mred\u001b[39m\u001b[33m'\u001b[39m, linestyle=\u001b[33m'\u001b[39m\u001b[33m--\u001b[39m\u001b[33m'\u001b[39m, label=\u001b[33m'\u001b[39m\u001b[33mAvg Subscription Rate\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m plt.title(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mSubscription Rate by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\seaborn\\categorical.py:2341\u001b[39m, in \u001b[36mbarplot\u001b[39m\u001b[34m(data, x, y, hue, order, hue_order, estimator, errorbar, n_boot, seed, units, weights, orient, color, palette, saturation, fill, hue_norm, width, dodge, gap, log_scale, native_scale, formatter, legend, capsize, err_kws, ci, errcolor, errwidth, ax, **kwargs)\u001b[39m\n\u001b[32m   2338\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mlen\u001b[39m:\n\u001b[32m   2339\u001b[39m     estimator = \u001b[33m\"\u001b[39m\u001b[33msize\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2341\u001b[39m p = \u001b[43m_CategoricalAggPlotter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2342\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2343\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munits\u001b[49m\u001b[43m=\u001b[49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2344\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2345\u001b[39m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[43m=\u001b[49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2346\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2347\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlegend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlegend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2348\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2350\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2351\u001b[39m     ax = plt.gca()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\seaborn\\categorical.py:67\u001b[39m, in \u001b[36m_CategoricalPlotter.__init__\u001b[39m\u001b[34m(self, data, variables, order, orient, require_numeric, color, legend)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     57\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     58\u001b[39m     data=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m     legend=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     65\u001b[39m ):\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# This method takes care of some bookkeeping that is necessary because the\u001b[39;00m\n\u001b[32m     70\u001b[39m     \u001b[38;5;66;03m# original categorical plots (prior to the 2021 refactor) had some rules that\u001b[39;00m\n\u001b[32m     71\u001b[39m     \u001b[38;5;66;03m# don't fit exactly into VectorPlotter logic. It may be wise to have a second\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m     \u001b[38;5;66;03m# default VectorPlotter rules. If we do decide to make orient part of the\u001b[39;00m\n\u001b[32m     77\u001b[39m     \u001b[38;5;66;03m# _base variable assignment, we'll want to figure out how to express that.\u001b[39;00m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.input_format == \u001b[33m\"\u001b[39m\u001b[33mwide\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m orient \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mh\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\seaborn\\_base.py:634\u001b[39m, in \u001b[36mVectorPlotter.__init__\u001b[39m\u001b[34m(self, data, variables)\u001b[39m\n\u001b[32m    629\u001b[39m \u001b[38;5;66;03m# var_ordered is relevant only for categorical axis variables, and may\u001b[39;00m\n\u001b[32m    630\u001b[39m \u001b[38;5;66;03m# be better handled by an internal axis information object that tracks\u001b[39;00m\n\u001b[32m    631\u001b[39m \u001b[38;5;66;03m# such information and is set up by the scale_* methods. The analogous\u001b[39;00m\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# information for numeric axes would be information about log scales.\u001b[39;00m\n\u001b[32m    633\u001b[39m \u001b[38;5;28mself\u001b[39m._var_ordered = {\u001b[33m\"\u001b[39m\u001b[33mx\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}  \u001b[38;5;66;03m# alt., used DefaultDict\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[38;5;66;03m# TODO Lots of tests assume that these are called to initialize the\u001b[39;00m\n\u001b[32m    637\u001b[39m \u001b[38;5;66;03m# mappings to default values on class initialization. I'd prefer to\u001b[39;00m\n\u001b[32m    638\u001b[39m \u001b[38;5;66;03m# move away from that and only have a mapping when explicitly called.\u001b[39;00m\n\u001b[32m    639\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mhue\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msize\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstyle\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\seaborn\\_base.py:679\u001b[39m, in \u001b[36mVectorPlotter.assign_variables\u001b[39m\u001b[34m(self, data, variables)\u001b[39m\n\u001b[32m    674\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    675\u001b[39m     \u001b[38;5;66;03m# When dealing with long-form input, use the newer PlotData\u001b[39;00m\n\u001b[32m    676\u001b[39m     \u001b[38;5;66;03m# object (internal but introduced for the objects interface)\u001b[39;00m\n\u001b[32m    677\u001b[39m     \u001b[38;5;66;03m# to centralize / standardize data consumption logic.\u001b[39;00m\n\u001b[32m    678\u001b[39m     \u001b[38;5;28mself\u001b[39m.input_format = \u001b[33m\"\u001b[39m\u001b[33mlong\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m679\u001b[39m     plot_data = \u001b[43mPlotData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    680\u001b[39m     frame = plot_data.frame\n\u001b[32m    681\u001b[39m     names = plot_data.names\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\seaborn\\_core\\data.py:58\u001b[39m, in \u001b[36mPlotData.__init__\u001b[39m\u001b[34m(self, data, variables)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     52\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     53\u001b[39m     data: DataSource,\n\u001b[32m     54\u001b[39m     variables: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, VariableSpec],\n\u001b[32m     55\u001b[39m ):\n\u001b[32m     57\u001b[39m     data = handle_data_source(data)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     frame, names, ids = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_assign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.frame = frame\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.names = names\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\seaborn\\_core\\data.py:232\u001b[39m, in \u001b[36mPlotData._assign_variables\u001b[39m\u001b[34m(self, data, variables)\u001b[39m\n\u001b[32m    230\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    231\u001b[39m         err += \u001b[33m\"\u001b[39m\u001b[33mAn entry with this name does not appear in `data`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    235\u001b[39m \n\u001b[32m    236\u001b[39m     \u001b[38;5;66;03m# Otherwise, assume the value somehow represents data\u001b[39;00m\n\u001b[32m    237\u001b[39m \n\u001b[32m    238\u001b[39m     \u001b[38;5;66;03m# Ignore empty data structures\u001b[39;00m\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(val) == \u001b[32m0\u001b[39m:\n",
      "\u001b[31mValueError\u001b[39m: Could not interpret value `job_type` for `x`. An entry with this name does not appear in `data`."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to plot subscription rates\n",
    "def plot_sub_rate(column, dataset):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    # Calculate percentage of 'yes' per category\n",
    "    sns.barplot(x=column, y='y', data=dataset, palette='viridis', errorbar=None) #ci=None)\n",
    "    plt.axhline(dataset['y'].mean(), color='red', linestyle='--', label='Avg Subscription Rate')\n",
    "    plt.title(f'Subscription Rate by {column}')\n",
    "    plt.ylabel('Proportion of \"Yes\"')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Test it on your job_type and your new financial flags\n",
    "for col in ['job_type', 'quarter', 'multiple_loans', 'is_new_customer']:\n",
    "    plot_sub_rate(col, df_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
