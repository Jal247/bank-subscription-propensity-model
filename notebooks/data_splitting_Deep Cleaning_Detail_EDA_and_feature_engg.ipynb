{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e039833",
   "metadata": {},
   "source": [
    "### Step 2. The Split (The Firewall): Split the data \n",
    "\n",
    "First will split data and then We will follow below steps to handle 'unknown' values from \"job', 'education','contact' and 'poutcome' columns.\n",
    "First, calculate the percentage of \"unknown\" values in each column to understand the extent of the issue.\n",
    "\n",
    "\"Pro-Tip\" for Banking\n",
    "When perform Train-Test Split, always use Stratification.\n",
    "\n",
    "In our bank-full dataset, \"Success\" (y=1) is likely rare (around 11-12%). If a random split done without stratification, it might end up with 15% success in Train and only 5% in Test. This will make evaluation metrics (like Precision/Recall) completely unreliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "26ccc78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the training data we processed earlier\n",
    "df = pd.read_csv('../data/processed/df_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22ebf16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e60a941d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (36168, 20)\n",
      "Test set size: (9043, 20)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Always stratify by the target variable 'y' in banking\n",
    "X = df.drop('y', axis=1)\n",
    "y = df['y']\n",
    "# 80/20 Split with Stratification (keeps failure ratio consistent)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474405ef",
   "metadata": {},
   "source": [
    "Now, we have\n",
    "\n",
    "Train data = X_train, y_train \n",
    "\n",
    "Test data = X_test, y_test\n",
    "\n",
    "Feature Engineering\n",
    "\n",
    "Now, will perform Data cleaning, feature engineering and EDA only on train data : X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1d6f96cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36168\n",
      "<class 'pandas.DataFrame'>\n",
      "Index: 36168 entries, 24001 to 44229\n",
      "Data columns (total 21 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   age              36168 non-null  int64  \n",
      " 1   job              36168 non-null  str    \n",
      " 2   marital          36168 non-null  str    \n",
      " 3   education        36168 non-null  str    \n",
      " 4   default          36168 non-null  str    \n",
      " 5   balance          36168 non-null  int64  \n",
      " 6   housing          36168 non-null  str    \n",
      " 7   loan             36168 non-null  str    \n",
      " 8   contact          36168 non-null  str    \n",
      " 9   day              36168 non-null  int64  \n",
      " 10  month            36168 non-null  str    \n",
      " 11  duration         36168 non-null  int64  \n",
      " 12  campaign         36168 non-null  int64  \n",
      " 13  pdays            36168 non-null  int64  \n",
      " 14  previous         36168 non-null  int64  \n",
      " 15  poutcome         36168 non-null  str    \n",
      " 16  total_contacts   36168 non-null  int64  \n",
      " 17  call_efficiency  36168 non-null  float64\n",
      " 18  multiple_loans   36168 non-null  int64  \n",
      " 19  is_new_customer  36168 non-null  int64  \n",
      " 20  y                36168 non-null  str    \n",
      "dtypes: float64(1), int64(10), str(10)\n",
      "memory usage: 6.1 MB\n"
     ]
    }
   ],
   "source": [
    "# Combine X and y back together for cleaning, featurization, EDA and Modeling storage\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Save to processed directory\n",
    "# train_full.to_csv('../data/processed/train_processed.csv', index=False)\n",
    "# test_full.to_csv('../data/processed/test_processed.csv', index=False)\n",
    "\n",
    "# print(\"Data cleaning and feature engineering complete. Files saved to data/processed/.\")\n",
    "\n",
    "\n",
    "print(df_train.shape[0])\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd1a291",
   "metadata": {},
   "source": [
    "### Part B Data Cleaning only on train data after spliting data in to train and test.\n",
    "\n",
    "1. Impute: Replace unknown in job or education with the mode.\n",
    "2. Handle Outliers: Cap the extreme balance values (Capping/Winsorization).\n",
    "\n",
    "**Step 3. Statistical Modeling Cleaning: Post-Split**\n",
    "\n",
    "Now in the Lab.\n",
    "\n",
    "- Imputation/Scaling: You calculate the \"Standard\" (Mean/Mode) from the Train set. If the Test set has a missing value, you fill           it with the Train mode. This mimics the real world, where you use past knowledge to handle new, incomplete information.\n",
    "\n",
    "- Handling Outliers\n",
    "\n",
    "- Bivariate EDA & Heatmaps: By doing this only on the training set, you ensure that your decision to keep or drop a feature is based only on the data the model is allowed to learn from.\n",
    "\n",
    "**Do these AFTER splitting:**\n",
    "\n",
    "1. Handling Outliers: Defining what an outlier is based on the training distribution.\n",
    "2. Imputation: Filling missing values using the mean, median, or mode of the training set only.\n",
    "3. Scaling/Normalization: If you normalize based on the global maximum, you've leaked the range of the test set into your training process.\n",
    "4. EDA: Visualizing correlations and distributions to decide which features to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2dfb1a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job            234\n",
      "marital          0\n",
      "education     1482\n",
      "default          0\n",
      "housing          0\n",
      "loan             0\n",
      "contact      10386\n",
      "month            0\n",
      "poutcome     29589\n",
      "y                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 4. Impute: Replace unknown in job or education with the mode.\n",
    "# selected only categorical columns\n",
    "\n",
    "categorical_columns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome', 'y']\n",
    "\n",
    "# count unknown values in each column\n",
    "\n",
    "unknown_counts = df_train[categorical_columns].apply(lambda col: (col == \"unknown\").sum())\n",
    "\n",
    "print(unknown_counts) # the output will give unknown counts for each categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "37f8ed64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: job, Unknown Values: 234, Percentage: 0.65%\n",
      "Column: education, Unknown Values: 1482, Percentage: 4.10%\n",
      "Column: poutcome, Unknown Values: 29589, Percentage: 81.81%\n",
      "Column: contact, Unknown Values: 10386, Percentage: 28.72%\n"
     ]
    }
   ],
   "source": [
    "# # Calculate the percentage of 'unknown' values in each relevant column\n",
    "columns_with_unknowns = ['job', 'education', 'poutcome', 'contact']\n",
    "for col in columns_with_unknowns:\n",
    "    unknown_count = df_train[df_train[col] == 'unknown'].shape[0]\n",
    "    total_count = df_train.shape[0]\n",
    "    percentage = (unknown_count / total_count) * 100\n",
    "    print(f\"Column: {col}, Unknown Values: {unknown_count}, Percentage: {percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7231be",
   "metadata": {},
   "source": [
    "**job and education:**  These are likely important features for predicting the target.\n",
    "the percentage of unknown values in 'job' and 'education' are 0.65% and 4.10%. which are very low.\n",
    "\n",
    "We will replace 'unknown' rows with mode value of that column.\n",
    "\n",
    "**contact and poutcome:**\n",
    "\n",
    "the percentage of unknown values in 'contact' and 'poutcome' are 28.72% and 81.81%. which are very high.\n",
    "\n",
    "'poutcome' columns have a high proportion (>30%) of \"unknown\" values and if their **impact** on the target variable (y) seems minimal, then we can remove them entirely.\n",
    "\n",
    "Let's check impact of 'contact' and 'potcome' on the target variable(y).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2601fb31",
   "metadata": {},
   "source": [
    "**Perform Chi-Square Test (Categorical Association)**\n",
    "\n",
    "A chi-square test can help determine whether there is a statistically significant association between the column (contact or poutcome) and the target variable (y).\n",
    "\n",
    "Interpretation:\n",
    "A p-value < 0.05 indicates a statistically significant relationship between the column and the target variable (y).\n",
    "A high p-value (>0.05) suggests the column has little impact on the target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "65cd644b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contact - Chi-square p-value: 2.2216425534984923e-177\n",
      "Poutcome - Chi-square p-value: 0.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Create contingency tables for 'contact' and 'poutcome'\n",
    "contact_table = pd.crosstab(df_train['contact'], df_train['y'])\n",
    "poutcome_table = pd.crosstab(df_train['poutcome'], df_train['y'])\n",
    "\n",
    "# Perform chi-square test\n",
    "contact_chi2, contact_p, _, _ = chi2_contingency(contact_table)\n",
    "poutcome_chi2, poutcome_p, _, _ = chi2_contingency(poutcome_table)\n",
    "\n",
    "print(f\"Contact - Chi-square p-value: {contact_p}\")\n",
    "print(f\"Poutcome - Chi-square p-value: {poutcome_p}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92dd692",
   "metadata": {},
   "source": [
    "**Perform Chi-Square Test (Categorical Association)**\n",
    "\n",
    "A chi-square test can help determine whether there is a statistically significant association between the column (contact or poutcome) and the target variable (y).\n",
    "\n",
    "Interpretation:\n",
    "A p-value < 0.05 indicates a statistically significant relationship between the column and the target variable (y).\n",
    "A high p-value (>0.05) suggests the column has little impact on the target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "110e4b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is significant association with  2.2216425534984923e-177\n",
      "there is significant association with  0.0\n"
     ]
    }
   ],
   "source": [
    "check= [contact_p, poutcome_p] \n",
    "\n",
    "for col in check:\n",
    "    if col >= 0.05:\n",
    "        print(\"there is no association with\", col)\n",
    "    else:\n",
    "        print(\"there is significant association with \", col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52844178",
   "metadata": {},
   "source": [
    "Conclusion: we will retain the 'contact' and 'poutcome' columns from the data df_train. \n",
    "We have two options: \n",
    "1. continue as is with 'unkown' values\n",
    "2. replace 'unknown' values by mode value of that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "16ff7f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode value of contact :  cellular\n",
      "mode value of poutcome:  unknown\n",
      "mode value of job :  blue-collar\n",
      "mode value of education:  secondary\n"
     ]
    }
   ],
   "source": [
    "# finding mode for 'contact' and 'potcome'\n",
    "mode_contact = df_train['contact'].mode()[0]\n",
    "mode_poutcome = df_train['poutcome'].mode()[0]\n",
    "\n",
    "print(\"mode value of contact : \", mode_contact)\n",
    "print(\"mode value of poutcome: \", mode_poutcome)\n",
    "\n",
    "# finding mode for 'job' and 'education'\n",
    "mode_job = df_train['job'].mode()[0]\n",
    "mode_education = df_train['education'].mode()[0]\n",
    "print(\"mode value of job : \", mode_job)\n",
    "print(\"mode value of education: \", mode_education)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b02d7896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning 'unknown' values imputed with relevant mode value in train and test data.\n"
     ]
    }
   ],
   "source": [
    "# Replacing 'unknow' values by relevant column's mode value\n",
    "# We will keep poutcome \"unknowm\" values as is because the model will learn that \"unknown\" actually means \"new customer.\"\n",
    "df_cleaned=df_train\n",
    "for col in ['job', 'education','contact']:\n",
    "    mode_value = df_train[col].mode()[0]\n",
    "    df_cleaned[col] = df_cleaned[col].replace('unknown',mode_value)\n",
    "    X_test[col]=X_test[col].replace('unknown',mode_value)\n",
    "    #print(df_cleaned[col])\n",
    "\n",
    "\n",
    "print(\"Cleaning 'unknown' values imputed with relevant mode value in train and test data.\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f78ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Rename 'unknown' in poutcome to keep the signal\n",
    "# This prevents 'unknown' from being treated as a missing value\n",
    "df_train['poutcome'] = df_train['poutcome'].replace('unknown', 'other_outcome')\n",
    "X_test['poutcome'] = X_test['poutcome'].replace('unknown', 'other_outcome')\n",
    "\n",
    "print(\"Imputation complete using Training Set statistics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3ca313",
   "metadata": {},
   "source": [
    "**Professional Outlier Handling: Capping**\n",
    "\n",
    "Handle Outliers: Cap the extreme balance values (Capping/Winsorization).\n",
    "\n",
    "Logic: don't just delete the outliers (that way we may lose too much data). Instead, cap them at the 99th percentile. This keeps the \"High Value\" signal without letting the â‚¬102,127 balance break your model.\n",
    "\n",
    "Capping vs. Deleting: In banking, we rarely delete outliers like a â‚¬102k balance. That person is a \"High Net Worth\" client! By Capping (using .clip()), we keep the customer in the dataset but prevent their high balance from \"pulling\" the average too far away from the typical customer.\n",
    "\n",
    "we want our \"Bivariate\" plots to be accurate. If we don't cap outliers first, our scatter plots will look like a single dot because of the scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e3994897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers capped at 99th percentile for balance, duration, and campaign.\n",
      "Cleaning Complete: outliers are capped and unkown value handled. \n",
      "Data is ready for Featurization(Feature Engineering) and EDA\n"
     ]
    }
   ],
   "source": [
    "# 5. Handle Outliers (Capping / Winsorization)\n",
    "#  Handle Outliers (capping): Cap the extreme balance values (Capping/Winsorization).\n",
    "for col in ['balance', 'duration', 'campaign']:\n",
    "    upper_limit = df_train[col].quantile(0.99)\n",
    "    # We cap the values at the 99th percentile\n",
    "    df_train[col] = np.where(df_train[col] > upper_limit, upper_limit, df_train[col])\n",
    "    #or \n",
    "    # df_train[col] = df_train[col].clip(upper=upper_limit)\n",
    "\n",
    "\n",
    "print(\"Outliers capped at 99th percentile for balance, duration, and campaign.\")\n",
    "print(\"Cleaning Complete: outliers are capped and unkown value handled. \\nData is ready for Featurization(Feature Engineering) and EDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d54642a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'job', 'marital', 'education', 'default', 'balance', 'housing',\n",
      "       'loan', 'contact', 'day', 'month', 'duration', 'campaign', 'pdays',\n",
      "       'previous', 'poutcome', 'total_contacts', 'call_efficiency',\n",
      "       'multiple_loans', 'is_new_customer', 'y'],\n",
      "      dtype='str')\n",
      "<class 'pandas.DataFrame'>\n",
      "Index: 36168 entries, 24001 to 44229\n",
      "Data columns (total 21 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   age              36168 non-null  int64  \n",
      " 1   job              36168 non-null  str    \n",
      " 2   marital          36168 non-null  str    \n",
      " 3   education        36168 non-null  str    \n",
      " 4   default          36168 non-null  str    \n",
      " 5   balance          36168 non-null  float64\n",
      " 6   housing          36168 non-null  str    \n",
      " 7   loan             36168 non-null  str    \n",
      " 8   contact          36168 non-null  str    \n",
      " 9   day              36168 non-null  int64  \n",
      " 10  month            36168 non-null  str    \n",
      " 11  duration         36168 non-null  float64\n",
      " 12  campaign         36168 non-null  float64\n",
      " 13  pdays            36168 non-null  int64  \n",
      " 14  previous         36168 non-null  int64  \n",
      " 15  poutcome         36168 non-null  str    \n",
      " 16  total_contacts   36168 non-null  int64  \n",
      " 17  call_efficiency  36168 non-null  float64\n",
      " 18  multiple_loans   36168 non-null  int64  \n",
      " 19  is_new_customer  36168 non-null  int64  \n",
      " 20  y                36168 non-null  str    \n",
      "dtypes: float64(4), int64(7), str(10)\n",
      "memory usage: 7.1 MB\n"
     ]
    }
   ],
   "source": [
    "print(df_train.columns)\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba53cb6",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "In a professional banking workflow, the order of Feature Engineering versus Bivariate EDA is a bit of a \"chicken and egg\" situation, but there is a preferred path for efficiency.\n",
    "\n",
    "The industry-standard approach is to do Feature Engineering before Bivariate EDA.\n",
    "\n",
    "Why this order?\n",
    "\n",
    "If you create a feature like balance_per_campaign or age_groups first, you can include them in your Bivariate EDA and Heatmaps. This allows you to see if your newly created features actually have a stronger relationship with the target (y) than the raw data did.\n",
    "\n",
    "Standard: Create new columns that are derived from row-level logic (e.g., balance_per_campaign or age_groups).\n",
    "\n",
    "Why: These are based on business rules, not statistical distributions.\n",
    "\n",
    "Status: Correct. You did this before the split.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9397055f",
   "metadata": {},
   "source": [
    "**Summary Checklist for our Notebook:**\n",
    "\n",
    "done: Cleaning: Fix the data (Mode imputation, Outlier capping).\n",
    "\n",
    "current: Feature Engineering: Add new columns (derived from your SQL logic).\n",
    "\n",
    "Next: EDA (Part B): Compare variables to y (Bivariate) and check correlations.\n",
    "\n",
    "Split: Separate your data.\n",
    "\n",
    "Scale: Normalize the numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bda9603",
   "metadata": {},
   "source": [
    "# Step 4: Feature Engineering\n",
    "\n",
    "Goal: Create new features or modify existing ones to improve model performance.\n",
    "\n",
    "Actions:\n",
    "\n",
    "- Group numerical values (e.g., age groups).\n",
    "- Create interaction features (e.g., balance_per_campaign).\n",
    "\n",
    "Created few interesting features interactions in sql and used those to interprit using visuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460c8b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "AR1 = pd.read_csv('../data/sql/age_range_analysis.csv',sep=',')  # reading the full dataset of bank\n",
    "AR2 = pd.read_csv('../data/sql/default_analysis.csv',sep=',')  # reading the full dataset of bank\n",
    "AR3 = pd.read_csv('../data/sql/housing_loan_analysis.csv',sep=',')  # reading the full dataset of bank\n",
    "AR4 = pd.read_csv('../data/sql/job_analysis.csv',sep=',')  # reading the full dataset of bank\n",
    "AR5 = pd.read_csv('../data/sql/marital_status_analysis.csv',sep=',')  # reading the full dataset of bank\n",
    "AR6 = pd.read_csv('../data/sql/personal_loan_analysis.csv',sep=',')  # reading the full dataset of bank\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d5b2b2",
   "metadata": {},
   "source": [
    "# Part B Exploratory Data Analysis \n",
    "\n",
    "Goal: Gain insights into the dataset and understand relationships between variables.\n",
    "\n",
    "we are now in the \"Discovery Phase.\" In a banking context, Bivariate EDA isn't just about pretty pictures; itâ€™s about finding Profitability Signalsâ€”identifying which types of customers are most likely to say \"Yes.\"\n",
    "\n",
    "Here is the industrial suite of Bivariate EDA, focusing on your engineered features and core banking variables.Actions:\n",
    "- Summarize numerical and categorical variables.\n",
    "- Visualize distributions, correlations, and trends.\n",
    "- Identify outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077538f4",
   "metadata": {},
   "source": [
    "1. Numerical Correlation (The \"Opportunity Map\")\n",
    "First, we look at the Heatmap. This tells us which variables move in sync with the target y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c3ac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Map target to binary for correlation purposes\n",
    "df_train['y_numeric'] = df_train['y'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "# 2. Select only numerical columns for the heatmap\n",
    "numerical_df = df_train.select_dtypes(include=['number'])\n",
    "\n",
    "# 3. Plot\n",
    "plt.figure(figsize=(6, 10))\n",
    "sns.heatmap(numerical_df.corr()[['y_numeric']].sort_values(by='y_numeric', ascending=False), \n",
    "            annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation: Features vs Subscription\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85eb96f",
   "metadata": {},
   "source": [
    "2. Categorical Analysis (The \"Customer Profile\")\n",
    "We want to see the Subscription Rate across different categories. This is where your structural features (like job_type or quarter) will shine.\n",
    "Since we have categorical columns like job, education, and poutcome, the best industrial practice is to use Stacked Bar Charts or Proportion Plots. This tells the bank: \"Which segment of people actually says yes?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b709f6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Bivariate: Percentage of success per category\n",
    "cat_cols = ['job', 'marital', 'education', 'contact', 'poutcome', 'is_new_customer']\n",
    "\n",
    "fig, axes = plt.subplots(len(cat_cols), 1, figsize=(10, 25))\n",
    "\n",
    "for i, col in enumerate(cat_cols):\n",
    "    # Calculate the percentage of 'yes' for each category\n",
    "    prop_df = df_train.groupby(col)['y'].value_counts(normalize=True).unstack().fillna(0)\n",
    "    prop_df.plot(kind='bar', stacked=True, ax=axes[i], color=['#ff9999','#66b3ff'])\n",
    "    axes[i].set_title(f'Subscription Proportion by {col}')\n",
    "    axes[i].set_ylabel('Proportion')\n",
    "    axes[i].legend(title='Subscribed', loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e2cd87",
   "metadata": {},
   "source": [
    "What to Look For in Your Results:\n",
    "is_new_customer (Your Feature): If the proportion of 'yes' is significantly higher for new customers than for those with pdays > 0, your engineering was a success.\n",
    "\n",
    "poutcome: In banking, \"Success\" in a previous campaign is usually the #1 predictor of success in the current one. This plot should confirm that.\n",
    "\n",
    "call_efficiency (In Heatmap): Check if this has a higher correlation than the raw duration. If it does, you've created a more \"dense\" signal for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef924f8",
   "metadata": {},
   "source": [
    "3. Numerical Distribution (The \"Behavioral Split\")\n",
    "How do \"Yes\" customers differ from \"No\" customers in terms of their balance or call duration? Because we capped the outliers earlier, these boxplots will actually be readable.\n",
    "\n",
    "Since you have already capped the outliers, this is the perfect time to visualize the \"gap\" in behavior. In a banking campaign, we look for displacement: does the \"Yes\" group significantly shift away from the \"No\" group on the X-axis?\n",
    "\n",
    "Here is the code to visualize the behavioral split for your numerical variables and your engineered features.\n",
    "\n",
    "ðŸ“Š Numerical Distribution Analysis (Bivariate)\n",
    "We will use Boxplots to see the quartiles and KDE Plots (Density) to see where the \"mass\" of the customers sits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4280e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Columns to analyze\n",
    "num_cols = ['balance', 'duration', 'call_efficiency', 'total_contacts']\n",
    "\n",
    "# Create a multi-plot grid\n",
    "fig, axes = plt.subplots(len(num_cols), 2, figsize=(15, 20))\n",
    "\n",
    "for i, col in enumerate(num_cols):\n",
    "    # 1. Boxplot - Shows medians and spreads clearly\n",
    "    sns.boxplot(x='y', y=col, data=df_train, ax=axes[i, 0], palette='Set2')\n",
    "    axes[i, 0].set_title(f'{col} Distribution by Subscription (Boxplot)')\n",
    "    \n",
    "    # 2. KDE Plot - Shows the \"overlap\" between Yes and No\n",
    "    sns.kdeplot(data=df_train, x=col, hue='y', common_norm=False, ax=axes[i, 1], fill=True)\n",
    "    axes[i, 1].set_title(f'{col} Density Split')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655d5ab6",
   "metadata": {},
   "source": [
    "How to Interpret the \"Behavioral Split\"\n",
    "1. Duration & Call Efficiency (The \"Engagement\" Signal)\n",
    "You will likely see that the \"Yes\" KDE plot is shifted significantly to the right.\n",
    "\n",
    "Industrial Insight: If the medians are far apart, these features are your \"Levers.\" The bank can practically use this to say, \"If a call lasts less than X seconds, the probability of conversion drops below 10%.\"\n",
    "\n",
    "2. Balance (The \"Capacity\" Signal)\n",
    "Even after capping, you might notice that the \"No\" group has a higher density at very low or negative balances.\n",
    "\n",
    "Industrial Insight: If the \"Yes\" boxplot is higher, it confirms that the product is reaching the intended affluent demographic.\n",
    "\n",
    "3. Total Contacts (The \"Fatigue\" Signal)\n",
    "Watch the \"Yes\" KDE for total_contacts. Usually, it peaks early (2-3 contacts) and then crashes.\n",
    "\n",
    "Industrial Insight: This helps you identify Diminishing Returns. If the density of \"Yes\" disappears after 5 contacts, the bank is wasting money calling people a 6th time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dc7fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing behavior across target\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Balance vs Y\n",
    "sns.boxplot(x='y', y='balance', data=df_train, ax=axes[0], palette='Set2')\n",
    "axes[0].set_title('Balance Distribution by Subscription')\n",
    "\n",
    "# Call Efficiency vs Y (Your engineered feature!)\n",
    "sns.boxplot(x='y', y='call_efficiency', data=df_train, ax=axes[1], palette='Set2')\n",
    "axes[1].set_title('Call Efficiency by Subscription')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517a641a",
   "metadata": {},
   "source": [
    "The \"Engagement\" Signal (duration & call_efficiency)\n",
    "The Gap: If the y=1 box is completely \"floating\" above the y=0 box, you have High Predictive Power.\n",
    "\n",
    "Bank Policy: You can recommend a \"Minimum Talk Time\" (MTT). If a salesperson can't keep a client on the phone for at least the \"Lower Whisker\" of the successful boxplot, they shouldn't bother recording it as a lead.\n",
    "\n",
    "B. The \"Capacity\" Signal (balance)\n",
    "The Floor: If the y=1 boxplot has a \"Lower Whisker\" that starts at â‚¬500, then anyone with less than â‚¬500 is technically an \"unqualified lead.\"\n",
    "\n",
    "Bank Policy: Use this as a Filter. Don't waste marketing dollars calling customers who fall into the bottom 25% of the y=0 balance distribution.\n",
    "\n",
    "C. The \"Fatigue\" Signal (total_contacts)\n",
    "The Crash Point: In your KDE plot, look at where the \"Yes\" line intersects the \"No\" line for the last time.\n",
    "\n",
    "Bank Policy: This is your Stop-Loss. If the conversion rate crashes after 4 contacts, you create a \"Hard Cap\" in the CRM system to prevent any customer from being called a 5th time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019a4632",
   "metadata": {},
   "source": [
    "4. Interactive Effects (The \"Marketing Sweet Spot\")\n",
    "Sometimes one variable behaves differently depending on another. Letâ€™s look at how duration affects success across different contact types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9766fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(x='contact', y='duration', hue='y', data=df_train, split=True, inner=\"quart\")\n",
    "plt.title('Impact of Contact Method and Duration on Success')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d80dd69",
   "metadata": {},
   "source": [
    "The Interaction Effect (Violin Plot Analysis)\n",
    "The violinplot of contact vs duration split by y is a powerful \"Efficiency Map.\"\n",
    "\n",
    "Cellular vs. Telephone: You might notice that 'cellular' calls have a different \"width\" (density) for success than 'telephone' (landline) calls.\n",
    "\n",
    "The \"Long Tail\" of Success: If the \"Yes\" (blue/green) side of the violin is much taller than the \"No\" side, it proves that Duration is a necessary condition for Success, but only for specific contact methods.\n",
    "\n",
    "Strategic Takeaway: If landlines require 10 minutes for a \"Yes\" but mobile calls only require 4 minutes, the bank should shift its budget to mobile-first campaigns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f969925",
   "metadata": {},
   "source": [
    "What to look for (The \"Senior\" Interpretation):\n",
    "The \"Red Line\" in Bar Plots: If a category (like 'Student' or 'Retired') is significantly above the red dashed line, that is a high-value segment for the bank.\n",
    "\n",
    "Call Efficiency: If your engineered call_efficiency shows a much tighter, higher distribution for \"Yes\" than \"No,\" you have successfully created a strong predictor.\n",
    "\n",
    "Negative Correlation: If multiple_loans has a strong negative correlation with y, it proves the \"Financial Stress\" hypothesisâ€”people with too much debt don't open savings accounts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca5dc2",
   "metadata": {},
   "source": [
    "One Final Check: Skewness\n",
    "If your KDE plots still look like a \"tall needle\" on the left with a very long tail, even after capping, it means the data is still highly skewed. In the next phase (Transformation), we might need to apply a Log Transformation or a Power Transformer to help the model \"see\" the data more clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427a41e6",
   "metadata": {},
   "source": [
    "3. Verification of Engineered Features\n",
    "Look closely at your call_efficiency boxplot.\n",
    "\n",
    "Does it show a cleaner separation than the raw duration boxplot?\n",
    "\n",
    "If the \"Yes\" and \"No\" boxes overlap less in call_efficiency than they do in duration, you have successfully reduced \"noise\" and created a feature that is easier for a Machine Learning model to \"split.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fde2a9e",
   "metadata": {},
   "source": [
    "**Saving data**\n",
    "\n",
    "In an industrial workflow, saving your cleaned and engineered training and testing sets separately is a standard requirement for Reproducibility.\n",
    "\n",
    "If you ever need to restart your kernel or hand this over to another teammate, you don't want to re-run the entire cleaning and engineering logic. You want to start exactly where you left off.\n",
    "\n",
    "ðŸ’¾ Saving the \"Checkpoint\"\n",
    "Since you have already done the hard work of handling unknowns, capping outliers, and creating new features, we will save these as .csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949a2c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Training Set (which contains the 'y' labels)\n",
    "#df_train.to_csv('../data/processed/bank_train_cleaned.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Save to processed directory\n",
    "# train_full.to_csv('../data/processed/train_processed.csv', index=False)\n",
    "# test_full.to_csv('../data/processed/test_processed.csv', index=False)\n",
    "\n",
    "# print(\"Data cleaning and feature engineering complete. Files saved to data/processed/.\")\n",
    "\n",
    "# Save the Test Set (ensuring it's in the same format)\n",
    "# If you haven't combined X_test and y_test yet:\n",
    "# df_test = X_test.copy()\n",
    "# df_test['y'] = y_test\n",
    "# df_test.to_csv('../data/processed/bank_test_cleaned.csv', index=False)\n",
    "\n",
    "# print(\"Files saved successfully: 'bank_train_cleaned.csv' and 'bank_test_cleaned.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffc7699",
   "metadata": {},
   "source": [
    "Step 5: Encoding & Scaling (The Model-Ready Matrix)\n",
    "Now we move to the final transformation. Because we have a mix of categories (Strings) and numbers (Floats/Ints), we use the ColumnTransformer.\n",
    "\n",
    "This tool is the industrial standard because it allows us to apply different transformations to different columns simultaneously while preventing Data Leakage.\n",
    "\n",
    "The Plan:\n",
    "One-Hot Encoding: For categorical columns (converts them to 0s and 1s).\n",
    "\n",
    "Standard Scaling: For numerical columns (centers data at 0 with a standard deviation of 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabd5fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Define which columns get which treatment\n",
    "num_features = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous', \n",
    "                'total_contacts', 'call_efficiency']\n",
    "\n",
    "cat_features = ['job', 'marital', 'education', 'default', 'housing', 'loan', \n",
    "                'contact', 'month', 'poutcome', 'age_group', 'quarter', \n",
    "                'job_type', 'is_new_customer', 'multiple_loans']\n",
    "\n",
    "# Create the transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_features),\n",
    "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), cat_features)\n",
    "    ])\n",
    "\n",
    "# FIT only on the Training data, then TRANSFORM both\n",
    "# This is where we ensure the \"Firewall\" is maintained\n",
    "X_train_final = preprocessor.fit_transform(X_train)\n",
    "X_test_final = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"Final shape of Training Data: {X_train_final.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
