{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "328a104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd  # pandas library\n",
    "import numpy as np   # numphy library\n",
    "import random        # random generation of number\n",
    "import os            # to see the path of the current file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87267a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age           job  marital  education default  balance housing loan  \\\n",
      "0   58    management  married   tertiary      no     2143     yes   no   \n",
      "1   44    technician   single  secondary      no       29     yes   no   \n",
      "2   33  entrepreneur  married  secondary      no        2     yes  yes   \n",
      "3   47   blue-collar  married    unknown      no     1506     yes   no   \n",
      "4   33       unknown   single    unknown      no        1      no   no   \n",
      "\n",
      "   contact  day month  duration  campaign  pdays  previous poutcome   y  \n",
      "0  unknown    5   may       261         1     -1         0  unknown  no  \n",
      "1  unknown    5   may       151         1     -1         0  unknown  no  \n",
      "2  unknown    5   may        76         1     -1         0  unknown  no  \n",
      "3  unknown    5   may        92         1     -1         0  unknown  no  \n",
      "4  unknown    5   may       198         1     -1         0  unknown  no  \n",
      "(45211, 17)\n",
      "   age          job  marital  education default  balance housing loan  \\\n",
      "0   30   unemployed  married    primary      no     1787      no   no   \n",
      "1   33     services  married  secondary      no     4789     yes  yes   \n",
      "2   35   management   single   tertiary      no     1350     yes   no   \n",
      "3   30   management  married   tertiary      no     1476     yes  yes   \n",
      "4   59  blue-collar  married  secondary      no        0     yes   no   \n",
      "\n",
      "    contact  day month  duration  campaign  pdays  previous poutcome   y  \n",
      "0  cellular   19   oct        79         1     -1         0  unknown  no  \n",
      "1  cellular   11   may       220         1    339         4  failure  no  \n",
      "2  cellular   16   apr       185         1    330         1  failure  no  \n",
      "3   unknown    3   jun       199         4     -1         0  unknown  no  \n",
      "4   unknown    5   may       226         1     -1         0  unknown  no  \n",
      "(4521, 17)\n"
     ]
    }
   ],
   "source": [
    "#Reading Dataset\n",
    "# Use lowercase 'data' to match your professional structure\n",
    "#df = pd.read_csv('../Data/raw/bank-full.csv', sep=',', encoding='latin1')\n",
    "df = pd.read_excel('../Data/raw/bank-full.xlsx')  # reading the full dataset of bank\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "# Reading the subset of full bank dataset\n",
    "df_sub = pd.read_csv('../Data/raw/bank.csv',sep=';')  # reading the subset(Sampling) dataset of bank\n",
    "print(df_sub.head())\n",
    "print(df_sub.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c0e02d",
   "metadata": {},
   "source": [
    "# Initial EDA : Univariate \n",
    "\n",
    "1. Identify the \"Problem\" Areas\n",
    "Before cleaning, you use Univariate EDA to find the things that need fixing.\n",
    "\n",
    "Numerical Outliers: You use boxplots to see if the €102k balance is an error or just a wealthy customer.\n",
    "\n",
    "Categorical \"Unknowns\": You use count plots to see that poutcome is mostly \"unknown.\" This justifies why you might drop it later or treat \"unknown\" as a category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09337753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of numerical columns that likely have outliers\n",
    "outlier_cols = ['age', 'balance', 'duration', 'campaign', 'previous']\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "for i, col in enumerate(outlier_cols):\n",
    "    plt.subplot(3, 2, i+1)\n",
    "    sns.boxplot(x=df[col], color='salmon')\n",
    "    plt.title(f'Outliers in {col}', fontsize=14)\n",
    "    plt.xlabel(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa94d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Visualize Balance Outliers\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.boxplot(x=df['balance'], color='orange')\n",
    "plt.title('Identifying Outliers in Customer Balance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2dca2d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Numerical Univariate Analysis**  (Use Histograms & Boxplots)\n",
    "\n",
    "For these columns, we are looking for skewness (is the data centered?) and outliers (like your €102k balance).\n",
    "Key Columns: age, balance, duration, campaign, pdays.\n",
    "\n",
    "Visualizations to use:\n",
    "\n",
    "Histograms: To see the distribution.\n",
    "\n",
    "Boxplots: Specifically to identify those outliers you mentioned in your describe() output.\n",
    "\n",
    "- age: To see the age range of the bank's customers.\n",
    "- balance: To identify wealth distribution and extreme outliers (like the €102k you found).\n",
    "- day: To see if calls are concentrated at the start, middle, or end of the month.\n",
    "- duration: Crucial variable. High duration usually correlates with success.\n",
    "- campaign: To see how many times a customer is usually contacted in one campaign.\n",
    "- pdays: Note: $-1$ means \"never contacted before.\" w'll see a huge spike at $-1$.\n",
    "- previous: To see the history of previous contacts.\n",
    "\n",
    "Check the \"natural shape\" of your continuous data.\n",
    "\n",
    "Age: Is the bank targeting retirees or young professionals?\n",
    "\n",
    "Balance: Is most of the data near zero? (In banking, this usually shows a \"long tail\" distribution).\n",
    "\n",
    "Duration: Check for 0-second calls. These are \"errors\" (missed calls) that you'll want to clean later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fac258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA - Part A: Univariate Analysis (Numerical Variables)\n",
    "\n",
    "# List of numerical columns\n",
    "num_columns = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
    "\n",
    "# Create subplots - 4 rows, 2 columns to fit all 7 variables\n",
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(16, 18))\n",
    "axes = axes.flatten() # Flatten to 1D array for easy indexing\n",
    "\n",
    "for i, col in enumerate(num_columns):\n",
    "    sns.histplot(df[col], bins=30, kde=True, ax=axes[i], color='teal')\n",
    "    axes[i].set_title(f'Distribution of {col}', fontsize=15)\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "# Remove the 8th empty subplot (since we only have 7 numerical columns)\n",
    "fig.delaxes(axes[7])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbba744",
   "metadata": {},
   "source": [
    "What to look for in these plots (Before Cleaning):\n",
    "age: Usually fairly normal but slightly right-skewed. You’ll see most customers are between 30 and 50.\n",
    "\n",
    "balance: This will likely look like a single vertical line at the left because of those extreme outliers (€100k+). This is your visual \"proof\" that you need to cap or scale this data later.\n",
    "\n",
    "duration: Heavily right-skewed. Most calls are short, but a few are very long. This is a very important predictor.\n",
    "\n",
    "campaign: Most values are at the very low end (1-3 contacts). The \"long tail\" shows people contacted 60+ times—often considered outliers.\n",
    "\n",
    "pdays: You will see a massive spike at -1. This represents all the customers who were never contacted before. In banking, this is a distinct \"New Lead\" segment.\n",
    "\n",
    "previous: Like campaign, mostly zeros. Any high numbers here are \"Frequent Contacts.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b994ff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
    "df[numerical_cols].hist(bins=20, figsize=(15, 10), color='skyblue', edgecolor='black')\n",
    "plt.suptitle('Univariate Analysis of Numerical Variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97364a2",
   "metadata": {},
   "source": [
    "**Categorical Univariate Analysis**\n",
    "\n",
    "For these, you are looking for frequency and imbalance. You want to see if one category dominates the others.\n",
    "\n",
    "Key Columns: job, marital, education, default, housing, loan, contact, month, poutcome.\n",
    "\n",
    "Visualizations to use:\n",
    "\n",
    "Count Plots (Bar Charts): To see the frequency of each category.\n",
    "\n",
    "What to look for: Does blue-collar make up 50% of the jobs? Is poutcome mostly \"unknown\"? (Which you already found is 81.75%!).\n",
    "\n",
    "- job: Which professions does the bank target most?\n",
    "- marital: Married, single, or divorced distribution.\n",
    "- education: Educational background of the leads.\n",
    "- default: Do many customers have credit in default? (Usually, most are \"no\").\n",
    "- housing: How many have housing loans?\n",
    "- loan: How many have personal loans?\n",
    "- contact: How the bank reached out (Cellular vs. Telephone).\n",
    "- month: Which months are the busiest for the bank?\n",
    "- poutcome: Success/Failure of previous campaigns (Remember, this had 81% \"unknown\")\n",
    "\n",
    "Look for \"Sparse Classes\" (categories with very few people).\n",
    "\n",
    "Job: If \"Student\" or \"Housemaid\" only has a few rows, your model might struggle to learn about them.\n",
    "\n",
    "Default: Does anyone actually have a credit default? If 99% are \"No,\" this column might not be useful for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5af31a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA - Part A: Univariate Analysis (Categorical Variables)\n",
    "\n",
    "# List of categorical columns from your info() output\n",
    "cat_columns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome', 'y']\n",
    "\n",
    "# Create subplots - 5 rows, 2 columns for the 10 categorical variables\n",
    "fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(16, 25))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(cat_columns):\n",
    "    # Sort bars by frequency for better readability\n",
    "    order = df[col].value_counts().index\n",
    "    sns.countplot(data=df, x=col, ax=axes[i], order=order, palette='viridis')\n",
    "    \n",
    "    axes[i].set_title(f'Frequency of {col}', fontsize=15)\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('Count')\n",
    "    \n",
    "    # Rotate x-labels for columns with many categories (like job and month)\n",
    "    if df[col].nunique() > 3:\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b34c2f",
   "metadata": {},
   "source": [
    "Crucial Banking Insights to Spot:\n",
    "Class Imbalance (The y chart): You will see \"no\" towers over \"yes.\" In a banking marketing context, this is normal but dangerous for a model. It means the model will naturally try to ignore the \"yes\" customers.\n",
    "\n",
    "The \"Unknown\" Dominance: Look at poutcome. You'll see \"unknown\" is the massive majority. This visually confirms your calculation that 81% of previous outcomes are missing.\n",
    "\n",
    "Dominant Segments: Notice how blue-collar and management jobs, and married status, make up the bulk of your leads.\n",
    "\n",
    "Credit Health: The default chart will likely be almost entirely \"no.\" This tells you that \"default\" might not have enough variance to be a strong predictor for your model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69da445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing a few key categories\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.countplot(data=df, x='job', order=df['job'].value_counts().index)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Job Distribution')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.countplot(data=df, x='education')\n",
    "plt.title('Education Levels')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc53f491",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8c77452",
   "metadata": {},
   "source": [
    "**The Target Variable (The Most Important One)**\n",
    "\n",
    "Even though y is categorical, we analyze it alone first to check for Class Imbalance.\n",
    "\n",
    "Column: y (Term Deposit Subscription).\n",
    "\n",
    "Insight: In this dataset, \"no\" usually happens about 88% of the time. This tells you that later on, you might need special modeling techniques (like SMOTE or class weights) because the \"yes\" cases are rare. \n",
    "\n",
    "y: This is the most important univariate analysis. We check for Class Imbalance. If 90% are \"no\" and 10% are \"yes,\" we need to note this as it affects our future model's accuracy.\n",
    "\n",
    "Check the distribution of y.\n",
    "\n",
    "Class Imbalance: In this dataset, you will likely see that ~88% said \"No\" and ~12% said \"Yes.\"\n",
    "\n",
    "The Cleaning Impact: By looking at this now, you'll know that if you delete too many \"unknown\" rows later, you might accidentally delete all your \"Yes\" cases!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4758a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Visualize the Target (Check for Imbalance)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='y', data=df, palette='viridis')\n",
    "plt.title('Baseline Target Distribution (Pre-Cleaning)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd522ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 3, 3)\n",
    "df['y'].value_counts().plot(kind='pie', autopct='%1.1f%%', colors=['lightcoral', 'lightgreen'])\n",
    "plt.title('Subscription (Target)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0710fca9",
   "metadata": {},
   "source": [
    "   # Step 2: Data Cleaning and Transformation:\n",
    "\n",
    "The initial step involves cleaning and preparing the raw data to remove inconsistencies, handle missing values, and make the dataset ready for analysis. \n",
    "\n",
    "1. Check for Missing Values: Ensure there are no null values.\n",
    "2. Handle Duplicates: Identify and remove any duplicate rows.\n",
    "3. Consistant Data for Analysis: Ensure consistent formats for categorical data (e.g., month values).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c410046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(age          0\n",
       " job          0\n",
       " marital      0\n",
       " education    0\n",
       " default      0\n",
       " balance      0\n",
       " housing      0\n",
       " loan         0\n",
       " contact      0\n",
       " day          0\n",
       " month        0\n",
       " duration     0\n",
       " campaign     0\n",
       " pdays        0\n",
       " previous     0\n",
       " poutcome     0\n",
       " y            0\n",
       " dtype: int64,\n",
       " age          0\n",
       " job          0\n",
       " marital      0\n",
       " education    0\n",
       " default      0\n",
       " balance      0\n",
       " housing      0\n",
       " loan         0\n",
       " contact      0\n",
       " day          0\n",
       " month        0\n",
       " duration     0\n",
       " campaign     0\n",
       " pdays        0\n",
       " previous     0\n",
       " poutcome     0\n",
       " y            0\n",
       " dtype: int64,\n",
       " np.int64(0),\n",
       " np.int64(0))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values in both datasets\n",
    "missing = df.isnull().sum()\n",
    "missing_sub = df_sub.isnull().sum()\n",
    "\n",
    "# Identify duplicate rows: Check for duplicates in both datasets\n",
    "duplicates = df.duplicated().sum()\n",
    "duplicates_sub = df_sub.duplicated().sum()\n",
    "\n",
    "missing, missing_sub, duplicates, duplicates_sub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82089e7",
   "metadata": {},
   "source": [
    "Looks like there are no missing values in our data. However, if we look at the categorical data there are 'unknown' values which we can consider as 'missing' data.\n",
    "\n",
    "Also, there are no duplicates rows in our data.\n",
    "We will follow below steps to handle 'unknown' values from \"job', 'education','contact' and 'poutcome' columns.\n",
    "First, calculate the percentage of \"unknown\" values in each column to understand the extent of the issue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "235ade28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job            288\n",
      "marital          0\n",
      "education     1857\n",
      "default          0\n",
      "housing          0\n",
      "loan             0\n",
      "contact      13020\n",
      "month            0\n",
      "poutcome     36959\n",
      "y                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# selected only categorical columns\n",
    "\n",
    "categorical_columns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome', 'y']\n",
    "\n",
    "# count unknown values in each column\n",
    "\n",
    "unknown_counts = df[categorical_columns].apply(lambda col: (col == \"unknown\").sum())\n",
    "\n",
    "print(unknown_counts) # the output will give unknown counts for each categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c47180d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: job, Unknown Values: 288, Percentage: 0.64%\n",
      "Column: education, Unknown Values: 1857, Percentage: 4.11%\n",
      "Column: poutcome, Unknown Values: 36959, Percentage: 81.75%\n",
      "Column: contact, Unknown Values: 13020, Percentage: 28.80%\n"
     ]
    }
   ],
   "source": [
    "# # Calculate the percentage of 'unknown' values in each relevant column\n",
    "columns_with_unknowns = ['job', 'education', 'poutcome', 'contact']\n",
    "for col in columns_with_unknowns:\n",
    "    unknown_count = df[df[col] == 'unknown'].shape[0]\n",
    "    total_count = df.shape[0]\n",
    "    percentage = (unknown_count / total_count) * 100\n",
    "    print(f\"Column: {col}, Unknown Values: {unknown_count}, Percentage: {percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2686d225",
   "metadata": {},
   "source": [
    "**job and education:**  These are likely important features for predicting the target.\n",
    "the percentage of unknown values in 'job' and 'education' are 0.64% and 4.11%. which are very low.\n",
    "\n",
    "We will replace 'unknown' rows with mode value of that column.\n",
    "\n",
    "**contact and poutcome:**\n",
    "\n",
    "These columns have a high proportion (>30%) of \"unknown\" values and if their **impact** on the target variable (y) seems minimal, then we can remove them entirely.\n",
    "\n",
    "Let's check impact of 'contact' and 'potcome' on the target variable(y).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9d77bb",
   "metadata": {},
   "source": [
    "**Perform Chi-Square Test (Categorical Association)**\n",
    "\n",
    "A chi-square test can help determine whether there is a statistically significant association between the column (contact or poutcome) and the target variable (y).\n",
    "\n",
    "Interpretation:\n",
    "A p-value < 0.05 indicates a statistically significant relationship between the column and the target variable (y).\n",
    "A high p-value (>0.05) suggests the column has little impact on the target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf8fc79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contact - Chi-square p-value: 1.251738325340638e-225\n",
      "Poutcome - Chi-square p-value: 0.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Create contingency tables for 'contact' and 'poutcome'\n",
    "contact_table = pd.crosstab(df['contact'], df['y'])\n",
    "poutcome_table = pd.crosstab(df['poutcome'], df['y'])\n",
    "\n",
    "# Perform chi-square test\n",
    "contact_chi2, contact_p, _, _ = chi2_contingency(contact_table)\n",
    "poutcome_chi2, poutcome_p, _, _ = chi2_contingency(poutcome_table)\n",
    "\n",
    "print(f\"Contact - Chi-square p-value: {contact_p}\")\n",
    "print(f\"Poutcome - Chi-square p-value: {poutcome_p}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73a4f5b",
   "metadata": {},
   "source": [
    "**Perform Chi-Square Test (Categorical Association)**\n",
    "\n",
    "A chi-square test can help determine whether there is a statistically significant association between the column (contact or poutcome) and the target variable (y).\n",
    "\n",
    "Interpretation:\n",
    "A p-value < 0.05 indicates a statistically significant relationship between the column and the target variable (y).\n",
    "A high p-value (>0.05) suggests the column has little impact on the target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a92357fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is significant association with  1.251738325340638e-225\n",
      "there is significant association with  0.0\n"
     ]
    }
   ],
   "source": [
    "check= [contact_p, poutcome_p] \n",
    "\n",
    "for col in check:\n",
    "    if col >= 0.05:\n",
    "        print(\"there is no association with\", col)\n",
    "    else:\n",
    "        print(\"there is significant association with \", col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1af7e8",
   "metadata": {},
   "source": [
    "Conclusion: we will retain the 'contact' and 'poutcome' columns from the data df. \n",
    "We have two options: \n",
    "1. continue as is with 'unkown' values\n",
    "2. replace 'unknown' values by mode value of that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "defa869f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode value of contact :  cellular\n",
      "mode value of poutcome:  unknown\n",
      "mode value of job :  blue-collar\n",
      "mode value of education:  secondary\n"
     ]
    }
   ],
   "source": [
    "# finding mode for 'contact' and 'potcome'\n",
    "mode_contact = df['contact'].mode()[0]\n",
    "mode_poutcome = df['poutcome'].mode()[0]\n",
    "\n",
    "print(\"mode value of contact : \", mode_contact)\n",
    "print(\"mode value of poutcome: \", mode_poutcome)\n",
    "\n",
    "# finding mode for 'job' and 'education'\n",
    "mode_job = df['job'].mode()[0]\n",
    "mode_education = df['education'].mode()[0]\n",
    "print(\"mode value of job : \", mode_job)\n",
    "print(\"mode value of education: \", mode_education)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5884fae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          management\n",
      "1          technician\n",
      "2        entrepreneur\n",
      "3         blue-collar\n",
      "4         blue-collar\n",
      "             ...     \n",
      "45206      technician\n",
      "45207         retired\n",
      "45208         retired\n",
      "45209     blue-collar\n",
      "45210    entrepreneur\n",
      "Name: job, Length: 45211, dtype: str\n",
      "0         tertiary\n",
      "1        secondary\n",
      "2        secondary\n",
      "3        secondary\n",
      "4        secondary\n",
      "           ...    \n",
      "45206     tertiary\n",
      "45207      primary\n",
      "45208    secondary\n",
      "45209    secondary\n",
      "45210    secondary\n",
      "Name: education, Length: 45211, dtype: str\n",
      "0         cellular\n",
      "1         cellular\n",
      "2         cellular\n",
      "3         cellular\n",
      "4         cellular\n",
      "           ...    \n",
      "45206     cellular\n",
      "45207     cellular\n",
      "45208     cellular\n",
      "45209    telephone\n",
      "45210     cellular\n",
      "Name: contact, Length: 45211, dtype: str\n"
     ]
    }
   ],
   "source": [
    "# Replacing 'unknow' values by relevant column's mode value\n",
    "# We will keep poutcome \"unknowm\" values as is because the model will learn that \"unknown\" actually means \"new customer.\"\n",
    "df_cleaned=df\n",
    "for col in ['job', 'education','contact']:\n",
    "    mode_value = df[col].mode()[0]\n",
    "    df_cleaned[col] = df_cleaned[col].replace('unknown',mode_value)\n",
    "    print(df_cleaned[col])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc3d753",
   "metadata": {},
   "source": [
    "**Professional Outlier Handling: Capping**\n",
    "\n",
    "don't just delete the outliers (you lose too much data). Instead, cap them at the 99th percentile. This keeps the \"High Value\" signal without letting the €102,127 balance break your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e901e8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Professional Outlier Handling: Capping\n",
    "for col in ['balance', 'duration', 'campaign']:\n",
    "    upper_limit = df[col].quantile(0.99)\n",
    "    # We cap the values at the 99th percentile\n",
    "    df[col] = np.where(df[col] > upper_limit, upper_limit, df[col])\n",
    "\n",
    "print(\"Outliers capped at 99th percentile for balance, duration, and campaign.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5390a04",
   "metadata": {},
   "source": [
    "After checking for missing and duplicate values our dataset is df_cleaned ready for Exploratory Data Analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06a5a37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
